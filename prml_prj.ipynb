{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prml_prj.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yXErjudQTBJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "%cd '/content/gdrive/My Drive/sem8/prml/prj'\n",
        "!ls\n",
        "current_loc=!pwd\n",
        "print(current_loc)\n",
        "from platform import python_version\n",
        "print(python_version())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58OEyLvgfxT-"
      },
      "source": [
        "#module imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import sklearn.cluster\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip3 install catboost\n",
        "import catboost\n",
        "from catboost import CatBoostRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQZgNL2reRIi"
      },
      "source": [
        "#data loads\n",
        "\n",
        "train = pd.read_csv('data/train.csv')\n",
        "songs = pd.read_csv('data/songs.csv')\n",
        "song_labels = pd.read_csv('data/song_labels.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "save_for_later = pd.read_csv('data/save_for_later.csv')\n",
        "dummy_submission = pd.read_csv('data/dummy_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P0dCtM6FKa4"
      },
      "source": [
        "#rough0\n",
        "\n",
        "\n",
        "#print(train[train['customer_id']=='O29219'])\n",
        "#print(song_labels[song_labels['label_id']==30574])\n",
        "#print(song_labels[song_labels['label_id']==8717]) #11557 11743\n",
        "\n",
        "#print(train.iloc[[8101]])\n",
        "#p=songs[songs['song_id']==5581]\n",
        "#print(p['count0'],p['count1'],p['count2'])\n",
        "#print(songs.iloc[[629]]['count0'],songs.iloc[[629]]['count1'],songs.iloc[[629]]['count2'])\n",
        "#0.0  4614.0  5117.0\n",
        "\n",
        "print(songs['released_year'].max())\n",
        "print(songs['language'].unique())\n",
        "#plt.plot(songs[songs['released_year']>=1860]]['released_year'])\n",
        "#pd.qcut(songs['released_year'], q=4)\n",
        "#language_mapping = {0: 'nan',1: 'eng',2:'en-US',3:'en-GB',4:'en-CA',}\n",
        "\n",
        "#mapping\n",
        "#Out: {1: 'A', 2: 'B', 3: 'C'}\n",
        "\n",
        "#sample_df['variable'].map(mapping)\n",
        "y = pd.get_dummies(songs['language'])\n",
        "#print('pp: ',songs[songs['language'].isnull()])\n",
        "print(songs['language'])\n",
        "print(y)\n",
        "#print('xi',(y.iloc[[3]]==0).all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnPPFTfAFNAp"
      },
      "source": [
        "#preprocessing songs\n",
        "\n",
        "max_labels=10\n",
        "song_labels_new = song_labels.groupby('label_id').sum().sort_values('count', ascending = False)\n",
        "song_labels_new.reset_index(inplace = True)\n",
        "principal_song_labels = song_labels_new['label_id'].to_numpy()[0:max_labels]  #highest labelled data is wantedly excluded!\n",
        "\n",
        "for i in range(len(principal_song_labels)):\n",
        "    label = song_labels[song_labels['label_id'] == principal_song_labels[i]]\n",
        "    label = label.reset_index().drop(['index', 'label_id'], axis = 1)\n",
        "    label[f'count{i}'] = label['count']\n",
        "    label.drop(['count'], axis = 1, inplace = True)\n",
        "    songs = pd.merge(songs, label, on = 'platform_id', how = 'left')\n",
        "    songs[f'count{i}'] = songs[f'count{i}'].fillna(0)\n",
        "    \n",
        "songs = songs.drop(['platform_id'], axis = 1)\n",
        "song_scores = train.merge(train.groupby('song_id').mean(), on = 'song_id', how = 'left')[['song_id', 'score_y']].drop_duplicates('song_id', keep = 'first')\n",
        "songs = songs.merge(song_scores, on = 'song_id', how = 'left')\n",
        "song_num_ratings = train['song_id'].value_counts().to_frame()\n",
        "song_num_ratings['num_ratings'] = song_num_ratings['song_id']\n",
        "song_num_ratings['song_id'] = song_num_ratings.index\n",
        "songs = songs.merge(song_num_ratings, on = 'song_id', how = 'left')\n",
        "songs.drop_duplicates('song_id', keep = 'first', inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLo0FSrun5Bd"
      },
      "source": [
        "#rough1\n",
        "\n",
        "print(principal_song_labels)\n",
        "print(song_labels)\n",
        "print(songs)\n",
        "print(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TteZBSOVFQKG"
      },
      "source": [
        "#train preprocessing\n",
        "\n",
        "f = pd.merge(train, save_for_later, on=['customer_id','song_id'], how='left', indicator='Exist')\n",
        "train = f\n",
        "\n",
        "X_train = pd.merge(train, songs, on = ['song_id'], how = 'left')\n",
        "Y_train = X_train['score']\n",
        "X_train.drop(['score'], axis = 1, inplace = True)\n",
        "X_train.drop('song_id', axis = 1, inplace = True)\n",
        "X_train['released_year']=abs(X_train['released_year'])\n",
        "X_train['released_year'] = X_train['released_year'].fillna(-999)\n",
        "X_train['language'] = X_train['language'].fillna('none')\n",
        "X_train['number_of_comments'] = X_train['number_of_comments'].fillna(-999)\n",
        "for i in range(max_labels):\n",
        "    X_train[f'count{i}'] = X_train[f'count{i}'].fillna(0)\n",
        "X_train['score_y']=X_train['score_y'].fillna(0)\n",
        "X_train['num_ratings']=X_train['num_ratings'].fillna(0)\n",
        "for i in range(max_labels):\n",
        "    X_train[f'count{i}'] = X_train[f'count{i}'].fillna(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOUXk-xRn8lO"
      },
      "source": [
        "#rough2\n",
        "\n",
        "\n",
        "X_train.loc[(X_train.released_year < 1860), 'released_year'] = -999\n",
        "print(X_train['released_year'])\n",
        "print('hi: ',np.where(np.array(X_train['released_year'])<1860))\n",
        "#X_train[:,X_train['released_year']<1860]['released_year']=-999\n",
        "#plt.plot(X_train['released_year'])\n",
        "#plt.show()\n",
        "#X_train[X_train['released_year'].min()==X_train['released_year']]\n",
        "#print(NaN in X_train['number_of_comments'].unique())\n",
        "print(principal_song_labels)\n",
        "print(X_train.columns)\n",
        "print(X_train['num_ratings'].isnull().values.any())\n",
        "print(X_train['Exist'])\n",
        "print('hell0: ',np.array(pd.qcut(X_train[X_train['customer_id']=='O29219']['released_year'], q=4).value_counts()))\n",
        "#X_train[1462:1464]\n",
        "#X_train['released_year'].min()<0\n",
        "#print(X_train['released_year'].min())\n",
        "#print(X_train['released_year'].max())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV6lTDjAFUGE"
      },
      "source": [
        "#feature vector compression\n",
        "epsilon=1e-9\n",
        "def softmax(z):  #computes softmax-values for each set of z along axis=-1\n",
        "    assert len(z.shape) == 2\n",
        "    s = np.max(z, axis=1)\n",
        "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
        "    e_x = np.exp(z - s)\n",
        "    div = np.sum(e_x, axis=1)\n",
        "    div = div[:, np.newaxis]\n",
        "    return e_x / div\n",
        "\n",
        "def squared_norm(z):  #computes squared-norm for each set of z along axis=-1\n",
        "    assert len(z.shape) == 2\n",
        "    return np.array(z)/(1e-9+np.linalg.norm(z,axis=-1))[:,np.newaxis]\n",
        "\n",
        "def norm(z):  #computes norm for each set of z along axis=-1\n",
        "    assert len(z.shape) == 2\n",
        "    return np.array(z)/(1e-9+np.sum(z,axis=-1))[:,np.newaxis]\n",
        "\n",
        "customers=X_train['customer_id'].unique()\n",
        "check,max_customers=0,len(customers)\n",
        "lang_one_hot=pd.get_dummies(X_train['language'])\n",
        "lang_one_hot['customer_id']=X_train['customer_id']\n",
        "compressed_features={}\n",
        "count_cols=[]\n",
        "for i in range(max_labels):\n",
        "    count_cols.append('count'+str(i))\n",
        "if check:\n",
        "    print('Xtrain: ',X_train)\n",
        "    print('lang_one_hot: ',lang_one_hot)\n",
        "    print('count_cols: ',count_cols)\n",
        "for customer in customers[0:max_customers]:\n",
        "    c=np.where(customers==customer)[0][0]\n",
        "    print('Processing ',customer,' : ',c,'/',len(customers))\n",
        "    customer_data=X_train[X_train['customer_id']==customer]\n",
        "    customer_scores=np.array(train[train['customer_id']==customer]['score'])\n",
        "    label_counts=squared_norm(np.array(customer_data[count_cols]))\n",
        "    year_cut=pd.cut(customer_data['released_year'], bins=[-1000, 1860, 1910, 1950, 2000, 2020])\n",
        "    year_counts=squared_norm(np.array([year_cut.value_counts()[1:]]))\n",
        "    lang_oh=np.array(lang_one_hot[lang_one_hot['customer_id']==customer].drop('customer_id', axis=1))\n",
        "    lang_counts=squared_norm(np.array([np.sum(lang_oh*(customer_scores[:,np.newaxis]),axis=0)]))\n",
        "    maxima_index=np.argmax(label_counts,axis=-1)\n",
        "    soft_labels=squared_norm(np.array([np.sum(label_counts*(customer_scores[:,np.newaxis]**2),axis=0)]))\n",
        "    if check:# or customer=='G23431':\n",
        "        print('label_counts: ',label_counts)\n",
        "        print('year_counts: ',year_counts)\n",
        "        print('score_data: ',customer_scores)\n",
        "        print('maxima_index: ',maxima_index)\n",
        "        print('year_cut: ',year_cut)\n",
        "        print('lang_one_hot: ',lang_oh)\n",
        "        print('lang_counts: ',lang_counts)\n",
        "        print('soft_labels: ',soft_labels)\n",
        "    compressed_features[customer]=np.concatenate((soft_labels[0],year_counts[0],lang_counts[0]))\n",
        "        \n",
        "print('compressed_features: ',compressed_features)\n",
        "#cust1=X_train[X_train['customer_id']=='O29219']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spNSW4-affwR"
      },
      "source": [
        "#rough 3\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "fm=np.array(list(compressed_features.values()))[0:10]\n",
        "print(fm.shape)\n",
        "pca = PCA(n_components=5)\n",
        "principalComponents = pca.fit_transform(fm)\n",
        "#pca.fit(fm)\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)\n",
        "print(principalComponents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwuv1ukuoAnl"
      },
      "source": [
        "#find best max_cluster to cluster datasets using hierarchical clustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import sklearn.cluster\n",
        "feature_matrix=np.array(list(compressed_features.values()))\n",
        "#feature_matrix_=cosine_similarity(feature_matrix)\n",
        "print(feature_matrix[0])\n",
        "#feature_matrix_=feature_matrix-feature_matrix.mean(axis=0)\n",
        "from sklearn.cluster import KMeans\n",
        "feature_matrix=np.array(list(compressed_features.values()))\n",
        "#feature_matrix_=cosine_similarity(feature_matrix)\n",
        "metrics1=[]\n",
        "metrics2=[]\n",
        "#metrics3=[]\n",
        "for i in range(2,30):\n",
        "    print('i: ',i)\n",
        "    clustering=sklearn.cluster.AgglomerativeClustering(n_clusters=i,affinity='cosine',linkage='average').fit(squared_norm(feature_matrix))\n",
        "    metrics1.append(metrics.silhouette_score(feature_matrix,clustering.labels_, metric='euclidean'))\n",
        "    metrics2.append(metrics.calinski_harabasz_score(feature_matrix,clustering.labels_))\n",
        "#    metrics3.append(kmeans.inertia_)\n",
        "print(clustering.labels_)\n",
        "#clustering=sklearn.cluster.AgglomerativeClustering(n_clusters=5).fit(feature_matrix)\n",
        "print(clustering.labels_.shape)\n",
        "#df=pd.DataFrame.from_dict(compressed_features,orient='index').transpose()\n",
        "#df.to_csv('data/names.csv')\n",
        "print(max(metrics1))\n",
        "print(metrics1)\n",
        "plt.plot(metrics1)\n",
        "plt.plot(metrics2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD8eIIhLie4d"
      },
      "source": [
        "#find best max_cluster to cluster datasets using kmeans clustering\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "feature_matrix=np.array(list(compressed_features.values()))\n",
        "#feature_matrix_=cosine_similarity(feature_matrix)\n",
        "metrics1=[]\n",
        "metrics2=[]\n",
        "metrics3=[]\n",
        "for i in range(2,30):\n",
        "    print('i: ',i)\n",
        "    kmeans = KMeans(n_clusters=i, random_state=0).fit(feature_matrix)\n",
        "    metrics1.append(metrics.silhouette_score(feature_matrix,kmeans.labels_, metric='euclidean'))\n",
        "    metrics2.append(metrics.calinski_harabasz_score(feature_matrix,kmeans.labels_))\n",
        "    metrics3.append(kmeans.inertia_)\n",
        "print(kmeans.labels_)\n",
        "print(max(metrics1))\n",
        "print(metrics1)\n",
        "plt.plot(metrics1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgmFXqpSoCBl"
      },
      "source": [
        "#grouping customers into cohorts\n",
        "\n",
        "max_clusters=2  #specify the max_cluster here after analyzing plots from above two cells (finding optimal cluster for hiearchical and k-means clustering)\n",
        "feature_matrix=np.array(list(compressed_features.values()))\n",
        "kmeans = KMeans(n_clusters=max_clusters, random_state=0).fit(feature_matrix)\n",
        "metrics.silhouette_score(feature_matrix,kmeans.labels_, metric='euclidean')\n",
        "metrics.calinski_harabasz_score(feature_matrix,kmeans.labels_)\n",
        "\n",
        "groups={}\n",
        "for label_id in range(len(kmeans.labels_)):\n",
        "    label=kmeans.labels_[label_id]\n",
        "    if label not in groups.keys():\n",
        "        groups[label]=[]\n",
        "    groups[label].append(customers[label_id])\n",
        "print('groups: ',groups)\n",
        "print('group keys: ',groups.keys())\n",
        "for i in range(max_clusters):\n",
        "    print('Length of cluster ',i,' : ',len(groups[i]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpL7zjWw9Pdo"
      },
      "source": [
        "#rough4 --to plot dendorgrams (tough to visualize due to large data)\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "# setting distance_threshold=0 ensures we compute the full tree.\n",
        "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
        "\n",
        "model = model.fit(feature_matrix)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(model, truncate_mode='level', p=3)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPqhDJ9DkXO0"
      },
      "source": [
        "#rough5\n",
        "\n",
        "#for i in range(10):\n",
        "#    plt.plot(X_trainee[X_trainee['customer_id']=='O29219']['count'+str(i)])\n",
        "#    plt.plot(X_trainee[X_trainee['customer_id']=='I50343']['count'+str(i)])\n",
        "#    plt.show()\n",
        "print(compressed_features['O29219'][0:10])\n",
        "print(compressed_features['I50343'][0:10])\n",
        "print(compressed_features['H45532'][0:10])\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import sklearn.cluster\n",
        "#feature_matrix=np.array(list(compressed_features.values()))\n",
        "cos1=cosine_similarity([compressed_features['O29219'][0:10]],[compressed_features['I50343'][0:10]])\n",
        "cos2=cosine_similarity([compressed_features['H45532'][0:10]],[compressed_features['I50343'][0:10]])\n",
        "cos3=cosine_similarity([compressed_features['O29219'][0:10]],[compressed_features['H45532'][0:10]])\n",
        "print(cos1,cos2,cos3)\n",
        "#print(X_trainee[X_trainee['customer_id']=='O29219']['count0'])\n",
        "#print(train[train['customer_id']=='O29219']['score'])\n",
        "#    print(X_trainee[X_trainee['customer_id']=='I50343']['count'+str(i)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJxzeNAxE954"
      },
      "source": [
        "#rough 6 --highly overfitting sklearn.tree!\n",
        "\n",
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeRegressor()\n",
        "#X_trainee=X_train[X_train.customer_id.isin(groups[0])]\n",
        "#X_trainee.drop('customer_id', axis = 1, inplace = True)\n",
        "#Y_trainee=train[train.customer_id.isin(groups[0])]['score']\n",
        "X_trainee=X_train\n",
        "Y_trainee=Y_train\n",
        "print(X_trainee['language'])\n",
        "\n",
        "X_trainee['customer_id']=X_trainee['customer_id'].astype('category')\n",
        "X_trainee['language']=X_trainee['language'].astype('category')\n",
        "cat_columns = X_trainee.select_dtypes(['category']).columns\n",
        "print(cat_columns)\n",
        "X_trainee[cat_columns] = X_trainee[cat_columns].apply(lambda x: x.cat.codes)\n",
        "print(X_trainee['language'])\n",
        "\n",
        "clf = clf.fit(X_trainee, Y_trainee)\n",
        "#clf.predict([[1, 1]])\n",
        "print(clf.predict(X_train))\n",
        "print(Y_train)\n",
        "loss = ((Y_train-clf.predict(X_train))**2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dllul4Mndf2M"
      },
      "source": [
        "class RmseMetric(object):  #metric used by catboostregressor (give as input argument to CatBoostRegressor)\n",
        "    def get_final_error(self, error, weight):\n",
        "        return np.sqrt(error / (weight + 1e-38))\n",
        "\n",
        "    def is_max_optimal(self):\n",
        "        return False\n",
        "\n",
        "    def evaluate(self, approxes, target, weight):\n",
        "        assert len(approxes) == 1\n",
        "        assert len(target) == len(approxes[0])\n",
        "\n",
        "        approx = approxes[0]\n",
        "\n",
        "        error_sum = 0.0\n",
        "        weight_sum = 0.0\n",
        "\n",
        "        for i in range(len(approx)):\n",
        "            w = 1.0 if weight is None else weight[i]\n",
        "            weight_sum += w\n",
        "            error_sum += w * ((approx[i] - target[i])**2)\n",
        "\n",
        "        return error_sum, weight_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEVUsN3OoG6a"
      },
      "source": [
        "#training pre-processing with multiple models for different groups\n",
        "models=[]\n",
        "for i in range(max_clusters):\n",
        "    print('Training model #',i)\n",
        "    X_trainee=X_train[X_train.customer_id.isin(groups[i])]\n",
        "#    X_trainee.drop('customer_id', axis = 1, inplace = True)\n",
        "    Y_trainee=train[train.customer_id.isin(groups[i])]['score']\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X_trainee, Y_trainee, train_size = 0.8)\n",
        "    model = CatBoostRegressor()#eval_metric=RmseMetric())\n",
        "    eval_dataset = catboost.Pool(data=x_test, label=y_test, cat_features=[0, 1, 3])\n",
        "    model.fit(x_train, y_train, eval_set = eval_dataset, cat_features = [0, 1,3], use_best_model = True, plot = True)\n",
        "    models.append(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5k2cSjFSa0D"
      },
      "source": [
        "#rough 7 --'sample weights=x' in model.fit()\n",
        "\n",
        "print(len(groups.keys()),groups[0])\n",
        "x=X_train.customer_id.isin(groups[0])\n",
        "print(x)\n",
        "x = x*1\n",
        "x=x+1\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzOfnFNTPHw5"
      },
      "source": [
        "#training pre-processing with one single model for the whole dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, train_size = 0.8)\n",
        "model = CatBoostRegressor(learning_rate=0.5,eval_metric=RmseMetric())\n",
        "#xxx=x_train.customer_id.isin(groups[0])\n",
        "#xxx = xxx*1\n",
        "#xxx=xxx+1\n",
        "eval_dataset = catboost.Pool(data=x_test, label=y_test, cat_features=[0, 1,3])  #[0, 1, 3])\n",
        "X_train.customer_id.isin(groups[i])\n",
        "model.fit(x_train, y_train, eval_set = eval_dataset, cat_features = [0, 1,3],use_best_model = True, plot = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnOhQwKmoJC5"
      },
      "source": [
        "#test pre-processing\n",
        "\n",
        "#test = pd.merge(test, save_for_later, on=['customer_id','song_id'], how='left', indicator='Exist')\n",
        "X_test = pd.merge(test, songs, on = ['song_id'], how = 'left')\n",
        "X_test.drop('song_id', axis = 1, inplace = True)\n",
        "#X_test.drop('customer_id', axis = 1, inplace = True)\n",
        "\n",
        "X_test['released_year'] = X_test['released_year'].fillna(-999)\n",
        "X_test['language'] = X_test['language'].fillna('none')\n",
        "X_test['number_of_comments'] = X_test['number_of_comments'].fillna(-999)\n",
        "print(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzYzg0TpLKsh"
      },
      "source": [
        "#rough 8 --clf (scikit tree) predictor for test\n",
        "X_test['customer_id']=X_test['customer_id'].astype('category')\n",
        "X_test['language']=X_test['language'].astype('category')\n",
        "print(X_test['language'])\n",
        "cat_columns = X_test.select_dtypes(['category']).columns\n",
        "print(cat_columns)\n",
        "X_test[cat_columns] = X_test[cat_columns].apply(lambda x: x.cat.codes)\n",
        "print(X_test['language'])\n",
        "print(X_test.columns)\n",
        "print(X_test['num_ratings'].isnull().values.any())\n",
        "X_test['score_y']=X_test['score_y'].fillna(0)\n",
        "X_test['num_ratings']=X_test['num_ratings'].fillna(0)\n",
        "for i in range(max_labels):\n",
        "    X_test[f'count{i}'] = X_test[f'count{i}'].fillna(0)\n",
        "\n",
        "print(clf.predict(X_test))\n",
        "print(Y_test)\n",
        "loss = ((Y_test-clf.predict(X_test))**2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_PPLrbvPYCs"
      },
      "source": [
        "#y_pred for multiple models (for multiple groups) case\n",
        "y_pred=np.zeros(len(X_test))\n",
        "for i in range(max_clusters):\n",
        "    print('i: ',i,'/',max_clusters)\n",
        "    X_testee=X_test[X_test.customer_id.isin(groups[i])]\n",
        "    y_pred[X_testee.index]=models[i].predict(X_testee)\n",
        "print('output length: ',len(y_pred))\n",
        "print(y_pred)\n",
        "#print(X_test.iloc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ2W0mgrFmlp"
      },
      "source": [
        "#rough 9\n",
        "\n",
        "test_customers=X_test['customer_id']\n",
        "y_test_pred=[]\n",
        "for c in range(len(test_customers)):\n",
        "    customer=test_customers[c]\n",
        "    print(c,'/',len(test_customers),':',customer)\n",
        "    ii=-1\n",
        "    for i in range(len(groups.keys())):\n",
        "        if customer in groups[i]:\n",
        "            ii=i\n",
        "            break\n",
        "    if ii==0:\n",
        "        y_test_pred.append(model0.predict(X_test.iloc[c]))\n",
        "    elif ii==1:\n",
        "        y_test_pred.append(model1.predict(X_test.iloc[c]))\n",
        "    elif ii==2:\n",
        "        y_test_pred.append(model2.predict(X_test.iloc[c]))\n",
        "    elif ii==3:\n",
        "        y_test_pred.append(model3.predict(X_test.iloc[c]))\n",
        "#    elif ii==4:\n",
        "#        y_test_pred.append(model4.predict(X_test.iloc[c]))\n",
        "    else:\n",
        "        print('issue!!')\n",
        "        break\n",
        "\n",
        "#y_test_pred = model.predict(X_test)M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hA7TyNDxRMr"
      },
      "source": [
        "#writing data to a file!\n",
        "#df=pd.DataFrame.from_dict(compressed_features,orient='index').transpose()\n",
        "#df.to_csv('data/names.csv')\n",
        "df=pd.DataFrame()\n",
        "#df['test_row_id']=np.arange(len(y_test_pred))\n",
        "df['score']=y_pred\n",
        "print(df.head())\n",
        "#x=pd.read_csv('./data/dummy_submission.csv')\n",
        "df.to_csv('data/ff.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}